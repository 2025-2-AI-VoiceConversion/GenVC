import torch
import time

@torch.inference_mode()
def handle_chunks(wav_gen, wav_gen_prev, wav_overlap, overlap_len=1024):
    """Handle chunk formatting in streaming mode"""
    wav_chunk = wav_gen[:-overlap_len]
    if wav_overlap is not None:
        # cross fade the overlap section
        if overlap_len > len(wav_chunk):
            wav_chunk = wav_gen[-overlap_len:]
            return wav_chunk, wav_gen, None
        else:
            crossfade_wav = wav_chunk[:overlap_len]
            crossfade_wav = crossfade_wav * torch.linspace(0.0, 1.0, overlap_len).to(crossfade_wav.device)
            wav_chunk[:overlap_len] = wav_overlap * torch.linspace(1.0, 0.0, overlap_len).to(wav_overlap.device)
            wav_chunk[:overlap_len] += crossfade_wav

    wav_overlap = wav_gen[-overlap_len:]
    wav_gen_prev = wav_gen
    return wav_chunk, wav_gen_prev, wav_overlap

@torch.inference_mode()
def synthesize_utt(
    genVC_mdl, 
    src_wav, 
    tgt_audio, 
    seg_len=6.0):
    """Synthesize audio in chunks, used for non-streaming mode
    The concatenation is performed at the latent feature level"""
    wav_gen_prev, wav_overlap = None, None
    total_wavlen = src_wav.shape[-1]
    pred_audios = []
    min_chunk_duration = int(0.32 * genVC_mdl.content_sample_rate)

    src_wav = src_wav.to(genVC_mdl.device)
    seg_len = int(seg_len * genVC_mdl.content_sample_rate)
    # get the conditioning latent
    tgt_audio = tgt_audio.to(genVC_mdl.device)
    cond_latent = genVC_mdl.get_gpt_cond_latents(tgt_audio, genVC_mdl.config.audio.sample_rate)
    final_latents = []

    for i in range(0, total_wavlen, seg_len):
        seg_end = i+seg_len if i+seg_len < total_wavlen else total_wavlen
        if seg_end == total_wavlen:
            src_wav_seg = src_wav[:, i:]
            if src_wav_seg.shape[-1] < min_chunk_duration:
                src_wav_seg = torch.nn.functional.pad(src_wav_seg, (0, min_chunk_duration-src_wav_seg.shape[-1]), "constant", 0)
        else:
            src_wav_seg = src_wav[:, i:i+seg_len]

        content_feat = genVC_mdl.content_extractor.extract_content_features(src_wav_seg)
        content_codes = genVC_mdl.content_dvae.get_codebook_indices(content_feat.transpose(1, 2))
        
        gen_codes = genVC_mdl.gpt.generate(
            cond_latent,
            content_codes,
            do_sample=True,
            top_p=genVC_mdl.config.top_p,
            top_k=genVC_mdl.config.top_k,
            temperature=genVC_mdl.config.temperature,
            num_beams=1,
            length_penalty=genVC_mdl.config.length_penalty,
            repetition_penalty=genVC_mdl.config.repetition_penalty,
            output_attentions=False,
        )[0]

        gen_codes = gen_codes[(gen_codes!=genVC_mdl.gpt.stop_audio_token).nonzero().squeeze()]
        expected_output_len = torch.tensor([gen_codes.shape[-1] * genVC_mdl.config.model_args.gpt_code_stride_len], device=genVC_mdl.device)
        content_len = torch.tensor([content_codes.shape[-1]], device=genVC_mdl.device)
        acoustic_latents = genVC_mdl.gpt(content_codes,
                                    content_len,
                                    gen_codes.unsqueeze(0),
                                    expected_output_len,
                                    cond_latents=cond_latent,
                                    return_latent=True)
        final_latents.append(acoustic_latents)
    
    # concatenate the latents
    final_latents = torch.cat(final_latents, dim=1)
    mel_input = torch.nn.functional.interpolate(
        final_latents.transpose(1, 2),
        scale_factor=[genVC_mdl.hifigan_scale_factor],
        mode="linear",
    ).squeeze(1)

    synthesized_audio = genVC_mdl.hifigan(mel_input)

    return synthesized_audio[0].squeeze()

@torch.inference_mode()
def synthesize_utt_chunked(
    genVC_mdl, 
    src_wav, 
    tgt_audio, 
    seg_len=6.0):
    """Synthesize audio in chunks, used for non-streaming mode
    The concatenation is performed at the waveform level"""
    wav_gen_prev, wav_overlap = None, None
    total_wavlen = src_wav.shape[-1]
    pred_audios = []
    min_chunk_duration = int(0.32 * genVC_mdl.content_sample_rate)

    src_wav = src_wav.to(genVC_mdl.device)
    seg_len = int(seg_len * genVC_mdl.content_sample_rate)
    # get the conditioning latent
    tgt_audio = tgt_audio.to(genVC_mdl.device)
    cond_latent = genVC_mdl.get_gpt_cond_latents(tgt_audio, genVC_mdl.config.audio.sample_rate)

    for i in range(0, total_wavlen, seg_len):
        seg_end = i+seg_len if i+seg_len < total_wavlen else total_wavlen
        if seg_end == total_wavlen:
            src_wav_seg = src_wav[:, i:]
            if src_wav_seg.shape[-1] < min_chunk_duration:
                src_wav_seg = torch.nn.functional.pad(src_wav_seg, (0, min_chunk_duration-src_wav_seg.shape[-1]), "constant", 0)
        else:
            src_wav_seg = src_wav[:, i:i+seg_len]
        audio_pred = genVC_mdl.inference(
            src_wav_seg, 
            cond_latent,
            top_p=genVC_mdl.config.top_p,
            top_k=genVC_mdl.config.top_k,
            temperature=genVC_mdl.config.temperature,
            length_penalty=genVC_mdl.config.length_penalty,
            repetition_penalty=genVC_mdl.config.repetition_penalty)
        
        wav_chunk, wav_gen_prev, wav_overlap = handle_chunks(
            audio_pred.squeeze(), wav_gen_prev, wav_overlap, 1024)
        pred_audios.append(wav_chunk)
    
    synthesized_audio = torch.cat(pred_audios, dim=-1)

    return synthesized_audio

@torch.inference_mode() # Do not modify
def synthesize_utt_streaming(
    genVC_mdl, 
    src_wav, 
    tgt_audio,
    seg_len=6.0,
    stream_chunk_size=8):

    wav_gen_prev, wav_overlap = None, None
    
    total_wavlen = src_wav.shape[-1]
    pred_audios = []
    min_chunk_duration = int(0.32 * genVC_mdl.content_sample_rate)

    begin_time = time.time()

    src_wav = src_wav.to(genVC_mdl.device)
    seg_len = int(seg_len * genVC_mdl.content_sample_rate)
    # get the conditioning latent
    tgt_audio = tgt_audio.to(genVC_mdl.device)
    cond_latent = genVC_mdl.get_gpt_cond_latents(tgt_audio, genVC_mdl.config.audio.sample_rate)
    is_begin = True
    for i in range(0, total_wavlen, seg_len):
        seg_end = i+seg_len if i+seg_len < total_wavlen else total_wavlen
        if seg_end == total_wavlen:
            src_wav_seg = src_wav[:, i:]
            if src_wav_seg.shape[-1] < min_chunk_duration:
                src_wav_seg = torch.nn.functional.pad(src_wav_seg, (0, min_chunk_duration-src_wav_seg.shape[-1]), "constant", 0)
        else:
            src_wav_seg = src_wav[:, i:i+seg_len]

        content_feat = genVC_mdl.content_extractor.extract_content_features(src_wav_seg)
        content_codes = genVC_mdl.content_dvae.get_codebook_indices(content_feat.transpose(1, 2))
        gpt_inputs = genVC_mdl.gpt.compute_embeddings(cond_latent, content_codes)

        gpt_generator = genVC_mdl.gpt.get_generator(
            fake_inputs=gpt_inputs,
            top_p=genVC_mdl.config.top_p,
            top_k=genVC_mdl.config.top_k,
            temperature=genVC_mdl.config.temperature,
            length_penalty=genVC_mdl.config.length_penalty,
            repetition_penalty=genVC_mdl.config.repetition_penalty,
            do_sample=True,
            num_beams=1,
            num_return_sequences=1,
            output_attentions=False,
            output_hidden_states=True,
        )

        last_tokens = []
        all_latents = []
        is_end = False
        while not is_end:
            try:
                x, latent = next(gpt_generator)
                last_tokens += [x]
                all_latents += [latent]
            except StopIteration:
                is_end = True

            if is_end or (stream_chunk_size > 0 and len(last_tokens) >= stream_chunk_size):
                acoustic_latents = torch.cat(all_latents, dim=0)[None, :]
                mel_input = torch.nn.functional.interpolate(
                    acoustic_latents.transpose(1, 2),
                    scale_factor=[genVC_mdl.hifigan_scale_factor],
                    mode="linear",
                ).squeeze(1)
                audio_pred = genVC_mdl.hifigan.forward(mel_input)
                wav_chunk, wav_gen_prev, wav_overlap = handle_chunks(
                    audio_pred.squeeze(), wav_gen_prev, wav_overlap, 1024)
                pred_audios.append(wav_chunk)
                last_tokens = []
                all_latents = []
                if is_begin:
                    is_begin = False
                    latency = time.time() - begin_time
                    print(f"Latency: {latency:.3f}s")
    
    synthesized_audio = torch.cat(pred_audios, dim=-1)
    processed_time = time.time() - begin_time
    real_time_factor = processed_time / (total_wavlen / genVC_mdl.content_sample_rate)
    print(f"Real-time factor: {real_time_factor:.3f}")
    return synthesized_audio


@torch.inference_mode()
def synthesize_utt_streaming_v2(
    genVC_mdl, 
    src_wav, 
    cond_latent,
    seg_len=6.0,
    stream_chunk_size=8):

    wav_gen_prev, wav_overlap = None, None
    
    total_wavlen = src_wav.shape[-1]
    pred_audios = []
    min_chunk_duration = int(0.32 * genVC_mdl.content_sample_rate)

    begin_time = time.time()

    src_wav = src_wav.to(genVC_mdl.device)
    seg_len = int(seg_len * genVC_mdl.content_sample_rate)
    is_begin = True
    for i in range(0, total_wavlen, seg_len):
        seg_end = i+seg_len if i+seg_len < total_wavlen else total_wavlen
        if seg_end == total_wavlen:
            src_wav_seg = src_wav[:, i:]
            if src_wav_seg.shape[-1] < min_chunk_duration:
                src_wav_seg = torch.nn.functional.pad(src_wav_seg, (0, min_chunk_duration-src_wav_seg.shape[-1]), "constant", 0)
        else:
            src_wav_seg = src_wav[:, i:i+seg_len]

        content_feat = genVC_mdl.content_extractor.extract_content_features(src_wav_seg)
        content_codes = genVC_mdl.content_dvae.get_codebook_indices(content_feat.transpose(1, 2))
        gpt_inputs = genVC_mdl.gpt.compute_embeddings(cond_latent, content_codes)

        gpt_generator = genVC_mdl.gpt.get_generator(
            fake_inputs=gpt_inputs,
            top_p=genVC_mdl.config.top_p,
            top_k=genVC_mdl.config.top_k,
            temperature=genVC_mdl.config.temperature,
            length_penalty=genVC_mdl.config.length_penalty,
            repetition_penalty=genVC_mdl.config.repetition_penalty,
            do_sample=True,
            num_beams=1,
            num_return_sequences=1,
            output_attentions=False,
            output_hidden_states=True,
        )

        last_tokens = []
        all_latents = []
        is_end = False
        while not is_end:
            try:
                x, latent = next(gpt_generator)
                last_tokens += [x]
                all_latents += [latent]
            except StopIteration:
                is_end = True

            if is_end or (stream_chunk_size > 0 and len(last_tokens) >= stream_chunk_size):
                acoustic_latents = torch.cat(all_latents, dim=0)[None, :]
                mel_input = torch.nn.functional.interpolate(
                    acoustic_latents.transpose(1, 2),
                    scale_factor=[genVC_mdl.hifigan_scale_factor],
                    mode="linear",
                ).squeeze(1)
                audio_pred = genVC_mdl.hifigan.forward(mel_input)
                wav_chunk, wav_gen_prev, wav_overlap = handle_chunks(
                    audio_pred.squeeze(), wav_gen_prev, wav_overlap, 1024)
                pred_audios.append(wav_chunk)
                last_tokens = []
                all_latents = []
                if is_begin:
                    is_begin = False
                    latency = time.time() - begin_time
                    print(f"Latency: {latency:.3f}s")
    
    synthesized_audio = torch.cat(pred_audios, dim=-1)
    processed_time = time.time() - begin_time
    real_time_factor = processed_time / (total_wavlen / genVC_mdl.content_sample_rate)
    print(f"Real-time factor: {real_time_factor:.3f}")
    return synthesized_audio

@torch.inference_mode() # Now use 
def synthesize_utt_streaming_mic(
    genVC_mdl, 
    content_token_sequence, # ìµœëŒ€ 6ì´ˆ ë¶„ëŸ‰ì˜ context token sequence 
    cond_latent, # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© : ì™¸ë¶€ì—ì„œ í•œë²ˆë§Œ ë¯¸ë¦¬ ê³„ì‚° (ì „ë‹¬ë§Œ ë°›ìŒ)
    stream_chunk_size=1,
    num_token=25, # í•œ ì²­í¬ê°€ ëª‡ê°œì˜ í† í°ì„ ë‹´ë‹¹í•˜ëŠ”ì§€ ì„¤ì • 
    wav_gen_prev=None, 
    wav_overlap=None,
    ):

    pred_audios = [] 

    '''
    [ë™ë£Œ ê°œë°œì êµ¬í˜„ ì˜ì—­]
    ì…ë ¥ë°›ì€ src_content(ì „ì²´ í† í° ì‹œí€€ìŠ¤)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
    ê°€ì¥ ìµœê·¼ ì²­í¬(1ì´ˆ)ì— í•´ë‹¹í•˜ëŠ” ìŒì„±ë§Œì„ ìƒì„±í•˜ì—¬ ë¦¬í„´í•´ì•¼ í•©ë‹ˆë‹¤.
    
    - Generator ìƒíƒœ ê´€ë¦¬ (Caching)
    - Look-ahead / Look-behind ì ìš©, Output Slicing [ë‹¨ ì´ë¶€ë¶„ì€ ë³´ì½”ë” ì¡°ì‚¬ í›„ pysunn êµ¬í˜„ ì˜ˆì •]
    ë“±ì˜ ë¡œì§ì´ ì´ê³³ì— êµ¬í˜„ë©ë‹ˆë‹¤.

    # í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ 
    min_chunk_duration = int(0.32 * genVC_mdl.content_sample_rate) # current not use
    ''' 
    
    # ì§€ê¸ˆ ìƒíƒœëŠ” í˜„ì¬ 1ì´ˆë‘ ê³¼ê±° 5ì´ˆì˜ í† í°ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

    begin_time = time.time()
    is_begin = True
    
    t_gpt_start = time.time()
    # ì´ chunk_size ê°œ ë§Œí¼ ë°˜ë³µí•©ë‹ˆë‹¤. ë§Œì•½ chunk_size = 2ë¼ë©´ ë‘ê°œì˜ ì²­í¬ì— ëŒ€í•´ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤. 
    for i in range(0, stream_chunk_size): 
        
        # ì„ë² ë”© ì œì‘ : [í™”ì í”„ë¡¬í”„íŠ¸, ë‚´ìš© ë¬¸ë§¥, START_AUDIO] 
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ êµ¬í˜„ ì—†ì´, ì´ì „ì— ë§Œë“¤ì—ˆë˜ ëª¨ë“  ì„ë² ë”©ì„ ì „ë¶€ ì €ì¥í•©ë‹ˆë‹¤. 
        gpt_inputs = genVC_mdl.gpt.compute_embeddings(cond_latent, content_token_sequence) 

        gpt_generator = genVC_mdl.gpt.get_generator( 
            fake_inputs=gpt_inputs,
            top_p=genVC_mdl.config.top_p,
            top_k=genVC_mdl.config.top_k,
            temperature=genVC_mdl.config.temperature,
            length_penalty=genVC_mdl.config.length_penalty,
            repetition_penalty=genVC_mdl.config.repetition_penalty,
            do_sample=True, # ì´ê±° Falseë¡œ í•˜ë©´ ìƒ˜í”Œ ìŠ¤íŠ¸ë¦¼ ì¶”ë¡  ì•ˆë˜ë”ë¼ 
            num_beams=1,
            num_return_sequences=1,
            output_attentions=False,
            output_hidden_states=True,
        )
        
        # 2. Generate Tokens
        all_latents = []
        last_tokens = []
        is_end = False

        t_gpt_start = time.time()
        while not is_end:
            try:
                x, latent = next(gpt_generator)
                last_tokens += [x]
                all_latents += [latent]
            except StopIteration:
                is_end = True
            
            # 8ê°œì˜ ìŒì„± í† í°ì„ GPTê°€ ë§Œë“¤ì–´ ë‚´ë©´, ë³´ì½”ë”ë¡œ ìŒì„± ì¡°ê°ì„ ë§Œë“¤ì–´ ë¦¬í„´í•œë‹¤ 
            if is_end or (num_token > 0 and len(last_tokens) >= num_token):
                t_gpt_end = time.time()
                
                acoustic_latents = torch.cat(all_latents, dim=0)[None, :]
                mel_input = torch.nn.functional.interpolate(
                    acoustic_latents.transpose(1, 2),
                    scale_factor=[genVC_mdl.hifigan_scale_factor],
                    mode="linear",
                ).squeeze(1)
                audio_pred = genVC_mdl.hifigan.forward(mel_input)
            
                t_vocoder_end = time.time()
                print(f"   [Detail] GPT: {t_gpt_end - t_gpt_start:.3f}s | Vocoder: {t_vocoder_end - t_gpt_end:.3f}s")

                # í¬ë¡œìŠ¤ í˜ì´ë”© ì•ˆí•¨ 
                wav_chunk = audio_pred.squeeze()
                # Cross-Fading ì ìš© (ì¼ë‹¨ì€ ë‚˜ì¤‘ì— ìƒê°) (ì§€ê¸ˆì€ ì²­í¬ ë¡ë¡ê±°ë¦¼ ìˆìŒ.)
                #wav_chunk, wav_gen_prev, wav_overlap = handle_chunks(
                #   audio_pred.squeeze(), wav_gen_prev, wav_overlap, overlap_len=1024
                #)
                
                pred_audios.append(wav_chunk)
                
                # Speak
                last_tokens = []
                all_latents = []
                
                if is_begin:
                    is_begin = False
                    latency = time.time() - begin_time
                    print(f"Latency: {latency:.3f}s")
                    
                # í•œ ì²­í¬ë§Œ ë§Œë“¤ê³  íƒˆì¶œ (ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì œì–´ìš©)
                break
                    
    # ì¼ë‹¨ ì§€ê¸ˆ êµ¬í˜„ì€ 6ì´ˆë§Œí¼ ìƒì„±í•˜ê³  ë°”ë¡œ ë„˜ê²¨ì£¼ëŠ” ì“°ë ˆê¸° êµ¬í˜„ì„.. 
    return pred_audios

@torch.inference_mode() 
def synthesize_utt_streaming_testflow(
    genVC_mdl, 
    input_tensor,
    cond_latent, 
    chunk_size, 
    past_key_values=None, 
    global_pos = 0,
    last_audio_token=None,
    ):
    """
    Stateful Streaming Inference Function
    
    Args:
        genVC_mdl: GenVC model instance
        input_tensor: Audio input [1, 1, S] (Includes past context + current chunk)
        cond_latent: Speaker style embedding
        chunk_size: The size of the 'new' audio chunk (in samples) to generate
        past_key_values: KV Cache from previous step
        global_pos: Current absolute position index for positional embedding
        last_audio_token: The last generated audio token from previous step
    
    Returns:
        wav_chunk (audio tensor), past_key_values, last_audio_token, global_pos
    """ 
    
    # =========================================================================
    # 0. Constants & Timing Setup
    # =========================================================================
    import time # ë”œë ˆì´ ë¡œê¹…ìš© 
    
    timing_log = {}
    t_total_start = time.time()
    
    GPT_CODE_STRIDE = 1024 
    tokens_to_generate = int(chunk_size / GPT_CODE_STRIDE) # chunk size ëŠ” 1024 ë°°ìˆ˜ì—¬ì•¼ í•¨ 
    
    # ì²­í¬ ì‚¬ì´ì¦ˆê°€ ë„ˆë¬´ ì‘ì•„ì„œ í† í°ì„ ë§Œë“¤ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì˜ˆì™¸ì²˜ë¦¬)
    if tokens_to_generate == 0:
        print("[Error] chunk size is too small")
        return None, past_key_values, last_audio_token, global_pos

    device = input_tensor.device
    gpt = genVC_mdl.gpt

    # =========================================================================
    # 1. Content Extraction [Audio Processing]
    # =========================================================================
    t1_start = time.time()
    
    # 1.1 Content Feature ì¶”ì¶œ 
    # Note: extract_content_features expects (batch, T) shape
    content_feat = genVC_mdl.content_extractor.extract_content_features(input_tensor)
    
    t1_feature = time.time()
    timing_log['1_feature_extraction'] = (t1_feature - t1_start) * 1000  # ms
    
    # 1.2 Content Code ì¶”ì¶œ (DVAE)
    full_codes = genVC_mdl.content_dvae.get_codebook_indices(content_feat.transpose(1, 2))
    
    t1_dvae = time.time()
    timing_log['2_dvae_quantization'] = (t1_dvae - t1_feature) * 1000  # ms

    # 1.3 Content Code ê°œìˆ˜ ê³„ì‚°í•˜ê¸° 
    '''
        If ì»¨í…ìŠ¤íŠ¸ê°€ ê½‰ ì°¬ ìƒíƒœ
        Else ì•„ì§ ê½‰ ì°¨ì§€ëŠ” ì•Šì€ ìƒíƒœ 
        ë¶„ê¸° ë‚˜ëˆ ì„œ ì •í™•íˆ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ì•¼í•¨. ì§€ê¸ˆ êµ¬í˜„ ë°”ë³´êµ¬í˜„ 
    '''
    
    # 1.4 ì´ë²ˆ ë‚´ìš©ì—ë§Œ ë”± ë§ëŠ” Content Code ìŠ¬ë¼ì´ì‹±
    # 3ì²­í¬ ì…ë ¥ ì¤‘ ë§¨ ë’¤(í˜„ì¬)ì— í•´ë‹¹í•˜ëŠ” í† í°ë§Œ ê°€ì ¸ì˜´
    # ì‹œê°„ì¶• ë™ê¸°í™”ë¥¼ ìœ„í•´ ì •í™•íˆ ê³„ì‚°ëœ ê°œìˆ˜ë§Œí¼ ë’¤ì—ì„œ ìë¦„.
    target_content_tokens = full_codes[:, -tokens_to_generate:]
    
    # =========================================================================
    # 2. GPT ì„ë² ë”© ì¤€ë¹„ [Cond + Target_Content + Audio Prompt]
    # =========================================================================
    '''
    ëª©í‘œ : ê¸°ì¡´ KV Cache ë’¤ì— ìƒˆë¡œìš´ Text ë¥¼ ë¶™ì¸ë‹¤. 
    ìƒí™© : [Prompt ... Content A Audio A] + [Content B] ë¥¼ ë¶™ì„ 
    '''

    # 2-1-1. Content ì„ë² ë”©
    txt_emb = gpt.text_embedding(target_content_tokens) # [B, T, Dim]

    # 2-1-2. Content Positional Embedding
    seq_len = target_content_tokens.shape[1]
    pos_ids_txt = torch.arange(global_pos, global_pos + seq_len, device=device) # ì–˜ëŠ” ë­ì„?
    
    # Positional Limit Clamping (í•™ìŠµëœ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²ƒì„ ë°©ì§€) 
    max_pos_txt = gpt.text_pos_embedding.emb.num_embeddings # ì–˜ëŠ” ë­ì„?

    if((global_pos + seq_len) >= max_pos_txt):
                print(f"[WARNING] Text Positional Limit Reached! Current: {global_pos + seq_len}, Max: {max_pos_txt}")

    pos_ids_txt = torch.clamp(pos_ids_txt, max=max_pos_txt-1) # ì–˜ëŠ” ë­ì„? 

    txt_pos = gpt.text_pos_embedding.emb(pos_ids_txt).unsqueeze(0)
    emb_content = txt_emb + txt_pos

    # 2-2 Input Embedding êµ¬ì„± 

    # 2-2-1. ìµœì´ˆ ì‹¤í–‰ 
    if past_key_values is None:
        inputs_embeds = torch.cat([cond_latent, emb_content], dim=1)
        # Start Token ì´ˆê¸°í™”
        last_audio_token = torch.tensor([[gpt.start_audio_token]], device=device)
    else:
        # [ìŠ¤íŠ¸ë¦¬ë° ì¤‘]: ì´ì „ KV Cache ë’¤ì— ì´ë²ˆ Contentë§Œ ë¶™ì„ 
        inputs_embeds = emb_content

    # 2-3. Forward (Text Prefill)
    t2_prefill_start = time.time()
    
    out = gpt.gpt(inputs_embeds=inputs_embeds, past_key_values=past_key_values, use_cache=True)
    past_key_values = out.past_key_values
    
    t2_prefill_end = time.time()
    timing_log['3_kv_prefill'] = (t2_prefill_end - t2_prefill_start) * 1000  # ms
    
    '''
    2.3 ì¶”ê°€ ì„¤ëª… )

    Before KVCache : [ìŠ¤íƒ€ì¼ + ì˜›ë‚  ë‚´ìš© + ì˜›ë‚  ìŒí–¥]
        [2.3 í¬ì›Œë“œ ì§„í–‰ í›„]
    After KVCache : [ìŠ¤íƒ€ì¼ + ì˜›ë‚  ë‚´ìš© + ì˜›ë‚  ìŒí–¥ + ì´ë²ˆ í…ìŠ¤íŠ¸ ë‚´ìš©]
    '''
    
    # 2-4. ê¸€ë¡œë²Œ ì»¤ì„œ ì—…ë°ì´íŠ¸ (ì¶”ê°€ëœ ë§Œí¼) 
    global_pos += inputs_embeds.shape[1] 

    '''
    3. GPT_Forward [Audio Generation]
    ëª©í‘œ: tokens_to_generate ë§Œí¼ ì˜¤ë””ì˜¤ í† í°ì„ ìƒì„±í•˜ë©´ ëœë‹¤. 
    '''

    # =========================================================================
    # 3. GPT_Forward [Audio Generation]
    # =========================================================================
    t3_gpt_start = time.time()
    
    curr_token = last_audio_token
    curr_pos = global_pos
    all_latents = [] # for Vocoder
    all_tokens = []
    # Mel Headì˜ Positional Embedding í•œê³„ 
    max_pos_mel = gpt.mel_pos_embedding.emb.num_embeddings

    # (Generation Loop) 
    for _ in range(tokens_to_generate):

        # 3.1.1 Mel Embedding 
        mel_emb = gpt.mel_embedding(curr_token)
        
        # 3.1.2. Positional Embedding 
        p_id = torch.tensor([curr_pos], device=device) # ì´ê²Œë­”ë°
    
        if(curr_pos >= max_pos_mel):
            print(f"[WARNING] Mel Positional Limit Reached! Current: {curr_pos}, Max: {max_pos_mel}")

        p_id = torch.clamp(p_id, max=max_pos_mel-1) #ì´ê²Œ ë­”ë°
        mel_pos = gpt.mel_pos_embedding.emb(p_id).unsqueeze(0) # ìœ„ì¹˜ ì„ë² ë”© ì–»ê¸° 

        # 3.1.3. Mel Input Embedding 
        curr_input = mel_emb + mel_pos 
        
        # 3.2. GPT Forward (Next Token Prediction)

        # ì´ì „ ê¸°ì–µ past_key_values ì™€ í˜„ì¬ ì…ë ¥ curr_input ì„ ë„£ëŠ”ë‹¤. 
        out = gpt.gpt(inputs_embeds=curr_input, past_key_values=past_key_values, use_cache=True)
        # ê¸°ì–µ ì—…ë°ì´íŠ¸ 
        past_key_values = out.past_key_values

        # 3.3. Decode 
        hidden = gpt.final_norm(out.last_hidden_state) # [1,1,Dim] ? 
        logits = gpt.mel_head(hidden) # íˆë“ ì—ì„œ ìŒì„± í—¤ë“œ êº¼ë‚´ê¸° 
        # * gpt.text_head(hidden) ë‚´ìš© í—¤ë“œêº¼ë‚´ë©´ pseudo context êµ¬í˜„ ê°€ëŠ¥í• ë“¯. 

        # =================================================================
        # [ğŸ›¡ï¸ Safety Net] ëª¨ë¸ ë©˜íƒˆ ìƒíƒœ ì ê²€ (Confidence & Entropy)
        # =================================================================
        
        # 1. í™•ë¥  ë¶„í¬ ê³„ì‚° (Softmax)
        probs = torch.nn.functional.softmax(logits, dim=-1) # [1, 1, Vocab]
        
        # 2. ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
        # (1) 1ë“± í† í°ê³¼ ê·¸ í™•ì‹ ë„(Confidence)
        top_prob, top_id = torch.max(probs, dim=-1)
        top_prob = top_prob.item() # 0.0 ~ 1.0
        
        # (2) Stop Token í™•ì‹ ë„
        stop_id = getattr(gpt, 'stop_audio_token', 8195)
        stop_prob = probs[0, 0, stop_id].item()
        
        # (3) ì—”íŠ¸ë¡œí”¼ (í˜¼ë€ë„) ê³„ì‚°
        # P * log(P)ì˜ í•©. ë†’ì„ìˆ˜ë¡ í˜¼ë€ìŠ¤ëŸ¬ì›€.
        # 1e-9ëŠ” log(0) ë°©ì§€ìš©
        entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1).item()

        print(f"Confidence: {top_prob:.4f}, Stop Confidence: {stop_prob:.4f}, Entropy: {entropy:.4f}")

        # stop token ë°©ì§€ 
        # stop_token_id = gpt.stop_audio_token
        # logits[:, :, stop_token_id] = -float('inf')

        # 3.4. Greedy Sampling 
        next_token = torch.argmax(logits, dim=-1) 
        all_tokens.append(next_token.item())

        # 3.5 Setup for Next Prediction 
        all_latents.append(hidden)
        curr_token = next_token
        curr_pos += 1

        # 3.6 Stop Check í•„ìš” 

        '''
        ëª¨ë¸ì´ ì¸í„°ë¦¬ë¹™ì„ ì˜ ì´í•´í•˜ì§€ ëª»í•˜ê³  ë°”ë¡œ end_tokenì„ ë±‰ëŠ” ìƒí™©ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ ê°€ëŠ¥ì„±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ 
        ''' 
        if next_token.item() == gpt.stop_audio_token:
            print("End Token reached...")
            #TODO: ì¤‘ë‹¨ëœ ìƒíƒœì—ì„œ í† í°ì„ ì–¼ë§ˆë‚˜ ë§ì´ ë§Œë“¤ì—ˆì—ˆëŠ”ì§€ ì¶œë ¥ 
            print(f"Generated {len(all_latents)}, {len(all_tokens)} tokens before end token. goal: {tokens_to_generate}")
            break
        
    t3_gpt_end = time.time()
    timing_log['4_gpt_generation'] = (t3_gpt_end - t3_gpt_start) * 1000  # ms
    timing_log['4_gpt_per_token'] = (t3_gpt_end - t3_gpt_start) * 1000 / max(len(all_latents), 1)  # ms/token
    
    last_audio_token = curr_token 
    # =========================================================================
    # 4. Sliding Window KVCache - êµ¬í˜„ í•´ì•¼í•¨. 
    # =========================================================================
    # ìºì‹œê°€ ë„ˆë¬´ ì»¤ì§€ë©´ OOM ë°©ì§€ë¥¼ ìœ„í•´ ì•ì„ ìë¦„

    NUM_STYLE_TOKENS = cond_latent.shape[1] if cond_latent is not None else 0
    KEEP_RECENT_TOKENS = 100

    MAX_WINDOW = NUM_STYLE_TOKENS + KEEP_RECENT_TOKENS
    
    #TODO: layer_past shape ë¡œê¹…ìœ¼ë¡œ ì‹¤ì œ ê²€ì¦ í™•ì¸í•˜ê¸° 
    if past_key_values is not None:
        # past_key_values[0]ì€ (Key, Value) íŠœí”Œì„
        # Key Shape: (Batch, Num_Heads, Seq_Len, Head_Dim) -> Index 2ê°€ Seq_Len
        current_seq_len = past_key_values[0][0].shape[2] 
        
        if current_seq_len > MAX_WINDOW:
            new_kv = []
            for layer_past in past_key_values: 
                # layer_past: (Key, Value)
                k, v = layer_past
                
                # 1. Key Pruning
                k_style = k[:, :, :NUM_STYLE_TOKENS, :]
                k_recent = k[:, :, -KEEP_RECENT_TOKENS:, :]
                k_pruned = torch.cat([k_style, k_recent], dim=2)
                
                # 2. Value Pruning
                v_style = v[:, :, :NUM_STYLE_TOKENS, :]
                v_recent = v[:, :, -KEEP_RECENT_TOKENS:, :]
                v_pruned = torch.cat([v_style, v_recent], dim=2)
                
                new_kv.append((k_pruned, v_pruned))
            
            past_key_values = tuple(new_kv)
        
    # =========================================================================
    # 5. Vocoding (HiFi-GAN)
    # =========================================================================
    t4_vocoder_start = time.time()

    if len(all_latents) == 0:
        print("Warning: No audio generated. Returning None.")
        return None, past_key_values, last_audio_token, global_pos

    # 5.1 Acoustic Latent ì œì‘  
    acoustic_latents = torch.cat(all_latents, dim=1) # [B, tokens_to_generate, Dim]
    
    '''
    gpt_code_stride_len = 1024 ë¡œ ìŒì„± í† í° 1ê°œë‹¹ ì˜¤ë””ì˜¤ 1024ìƒ˜í”Œì´ë‹¤.
    hop_length = 256 ìœ¼ë¡œ í•˜ì´íŒŒì´ê°  í™‰ ì‚¬ì´ì¦ˆëŠ” 256.
    ì¦‰ 4ë°°ì˜ ì‹œê°„ í•´ìƒë„ ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤. 
    1GPT token = 4 Mel frame ì´ë‹¤. 

    ì›ë³¸ GPT í† í°:    [A]             [B]                 [C]          [D]
                       â†“                â†“                â†“           â†“
    ë³´ê°„ í›„:        [A] [a1][a2][a3][B][b1][b2][b3][C][c1][c2][c3][D]...
                      â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”˜
                      4 frames         4 frames      4 frames
    '''
    # 5.2 ì„ í˜• ë³´ê°„ì„ í™œìš©í•´ì„œ Mel Input (to Vocoder)
    mel_input = torch.nn.functional.interpolate(
        acoustic_latents.transpose(1, 2),
        scale_factor=[genVC_mdl.hifigan_scale_factor],
        mode="linear",
    ).squeeze(1)
    
    # 5.3 Hifi-GAN ìŒì„± í•©ì„± 
    wav_chunk = genVC_mdl.hifigan.forward(mel_input).squeeze()
    
    t4_vocoder_end = time.time()
    timing_log['5_vocoding'] = (t4_vocoder_end - t4_vocoder_start) * 1000  # ms
    
    # =========================================================================
    # 6. Timing Summary
    # =========================================================================
    t_total_end = time.time()
    timing_log['total_time'] = (t_total_end - t_total_start) * 1000  # ms
    
    # ë¡œê·¸ ì¶œë ¥ (Fast I/O)
    import sys
    
    # RTF ê³„ì‚° (Real-Time Factor)
    audio_duration_ms = (chunk_size / 24000) * 1000  # 24kHz sample rate
    rtf = timing_log['total_time'] / audio_duration_ms
    
    # í•œ ë²ˆì— ë¬¸ìì—´ ìƒì„± í›„ ì¶œë ¥ (ë²„í¼ë§ ìµœì†Œí™”)
    log_output = (
        f"\n[â±ï¸  Timing Log - Chunk {chunk_size} samples]\n"
        f"  1ï¸âƒ£  Feature Extraction:  {timing_log['1_feature_extraction']:6.2f} ms\n"
        f"  2ï¸âƒ£  DVAE Quantization:   {timing_log['2_dvae_quantization']:6.2f} ms\n"
        f"  3ï¸âƒ£  KV Cache Prefill:    {timing_log['3_kv_prefill']:6.2f} ms\n"
        f"  4ï¸âƒ£  GPT Generation:      {timing_log['4_gpt_generation']:6.2f} ms ({timing_log['4_gpt_per_token']:.2f} ms/token)\n"
        f"  5ï¸âƒ£  Vocoding (HiFiGAN):  {timing_log['5_vocoding']:6.2f} ms\n"
        f"  {'â”€'*50}\n"
        f"  ğŸ”¥ Total:               {timing_log['total_time']:6.2f} ms\n"
        f"  ğŸ“Š RTF (Real-Time Factor): {rtf:.3f}\n\n"
    )
    
    sys.stdout.write(log_output)
    sys.stdout.flush()
    
    
    # 5.5 Cross-Fading Overlap êµ¬í˜„
    '''
    wav_chunk, wav_gen_prev, wav_overlap = handle_chunks(
        wav_chunk, wav_gen_prev, wav_overlap, overlap_len=1024
    )
    '''             
    return wav_chunk, past_key_values, last_audio_token, global_pos 

